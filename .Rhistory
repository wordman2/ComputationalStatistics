set.seed(1)
load("data/prostate.rdata")
library(glmnet)
library(ggplot2)
# lineare Regression
lm_model <- lm(lpsa~lcavol+lweight+age+lbph+svi+lcp+gleason+pgg45, data=prostate)
lm_coef <- coef(lm_model)
#ridge Regression
y <- prostate$lpsa
x <- data.matrix(prostate[,c('lcavol','lweight','age','lbph','svi','lcp','gleason','pgg45')])
rr_model_lambda_0 <- glmnet(x,y,alpha=0,lambda=0) # alpha=0 => Ridge Regression
rr_model_lambda_10 <- glmnet(x,y,alpha=0,lambda=10)
result <- data.frame(Lineare.Regression=lm_model$coefficients,
Ridge.Regression.lambda0=as.vector(coef(rr_model_lambda_0)),
Ridge.Regression.lambda10=as.vector(coef(rr_model_lambda_10)))
print(result)
rr_model <- glmnet(x,y,alpha=0)
plot(rr_model, xvar="lambda")
legend("topright", legend = rownames(coef(rr_model_lambda_10)), col = 1:nrow(coef(rr_model_lambda_10)), lty = 1)
lambdas <- c(0, 0.09, 2)
mse_estimates <- c()
for(lambda in lambdas){
train_x <- x[prostate$train==TRUE,]
train_y <- y[prostate$train==TRUE]
test_x <- x[prostate$train==FALSE,]
test_y <- y[prostate$train==FALSE]
rr_model <- glmnet(train_x,train_y,alpha=0, lambda=lambda)
estimates <- predict(rr_model,test_x)
mse <- sum((test_y-estimates)**2)
mse_estimates <- c(mse_estimates, mse)
}
# plot MSEs
ggplot(data.frame(MSE=round(mse_estimates, 4)), aes(x = as.character(lambdas), y = MSE)) +
geom_bar(stat = "identity") +
xlab("位") +
ylab("MSE") +
ggtitle("Mean Squared Error (MSE)") +
geom_text(aes(label=MSE), vjust=-0.5)
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
#produce plot of test MSE by lambda value
plot(cv_model)
set.seed(1)
rr_model_best_lambda <- glmnet(train_x, train_y, alpha = 0, lambda=best_lambda)
estimates <- predict(rr_model_best_lambda, test_x)
mse <- sum((test_y-estimates)**2)
mse_estimates <- c(mse_estimates, mse)
# plot MSEs
ggplot(data.frame(MSE=round(mse_estimates, 4)), aes(x = as.character(c(lambdas, "0.08")), y = MSE)) +
geom_bar(stat = "identity") +
xlab("位") +
ylab("MSE") +
ggtitle("Mean Squared Error (MSE)") +
geom_text(aes(label=MSE), vjust=-0.5)
data.frame(Koeffizienten.LineareRegression=coef(lm_model),Koeffizienten.best_lambda=as.vector(coef(rr_model_best_lambda)))
lasso_model_lambda_0 <- glmnet(x,y,alpha=1,lambda=0) # alpha=1 => Lasso-Verfahren
lasso_model_lambda_10 <- glmnet(x,y,alpha=1,lambda=10)
result <- data.frame(Lineare.Regression=lm_model$coefficients,
Ridge.Regression.lambda0=as.vector(coef(rr_model_lambda_0)),
Ridge.Regression.lambda10=as.vector(coef(rr_model_lambda_10)),
Lasso.lambda0=as.vector(coef(lasso_model_lambda_0)),
Lasso.lambda10=as.vector(coef(lasso_model_lambda_10)))
print(result)
lasso_model <- glmnet(x,y,alpha=1, lambda=seq(0,1,0.001))
plot(lasso_model, xvar="lambda")
legend("topright", legend = rownames(coef(lasso_model_lambda_0)), col = 1:nrow(coef(lasso_model_lambda_0)), lty = 1)
lambdas <- c(0, 0.002, 1)
mse_estimates <- c()
# split into training und test
train_x <- x[prostate$train==TRUE,]
train_y <- y[prostate$train==TRUE]
test_x <- x[prostate$train==FALSE,]
test_y <- y[prostate$train==FALSE]
for(lambda in lambdas){
lasso_model <- glmnet(train_x,train_y,alpha=1, lambda=lambda)
estimates <- predict(lasso_model,test_x)
mse <- sum((test_y-estimates)**2)
mse_estimates <- c(mse_estimates, mse)
}
# plot MSEs
ggplot(data.frame(MSE=round(mse_estimates, 4)), aes(x = as.character(lambdas), y = MSE)) +
geom_bar(stat = "identity") +
xlab("位") +
ylab("MSE") +
ggtitle("Mean Squared Error (MSE)") +
geom_text(aes(label=MSE), vjust=-0.5)
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda
#produce plot of test MSE by lambda value
plot(cv_model)
lasso_model_best_lambda <- glmnet(train_x, train_y, alpha = 1, lambda=best_lambda)
estimates <- predict(lasso_model_best_lambda, test_x)
mse <- sum((test_y-estimates)**2)
mse_estimates <- c(mse_estimates, mse)
# plot MSEs
ggplot(data.frame(MSE=round(mse_estimates, 4)), aes(x = as.character(c(lambdas, as.character(round(best_lambda,3)))), y = MSE)) +
geom_bar(stat = "identity") +
xlab("位") +
ylab("MSE") +
ggtitle("Mean Squared Error (MSE)") +
geom_text(aes(label=MSE), vjust=-0.5)
